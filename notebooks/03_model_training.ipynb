{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec8c571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Modeling libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Model evaluation\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score, \n",
    "                            roc_auc_score, confusion_matrix, classification_report, \n",
    "                            precision_recall_curve, roc_curve, average_precision_score)\n",
    "from sklearn.model_selection import (train_test_split, cross_val_score, \n",
    "                                    StratifiedKFold, RandomizedSearchCV, GridSearchCV)\n",
    "\n",
    "# For addressing class imbalance\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# For model explanation\n",
    "import shap\n",
    "from sklearn.inspection import permutation_importance\n",
    "from pdpbox import pdp, info_plots\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import ListedColormap, Normalize\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "%matplotlib inline\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0a161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for model evaluation and visualization\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba=None, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate a classification model using various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_pred_proba : array-like, optional\n",
    "        Predicted probabilities for the positive class\n",
    "    model_name : str, default=\"Model\"\n",
    "        Name of the model for display purposes\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Calculate AUC if probabilities are provided\n",
    "    auc_roc = None\n",
    "    avg_precision = None\n",
    "    if y_pred_proba is not None:\n",
    "        auc_roc = roc_auc_score(y_true, y_pred_proba)\n",
    "        avg_precision = average_precision_score(y_true, y_pred_proba)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Evaluation Results for {model_name}:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    \n",
    "    if auc_roc is not None:\n",
    "        print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "        print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    \n",
    "    # Return metrics as dictionary\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    if auc_roc is not None:\n",
    "        metrics['auc_roc'] = auc_roc\n",
    "        metrics['avg_precision'] = avg_precision\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(cm, classes=None, normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    Plot a confusion matrix.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    cm : array-like\n",
    "        Confusion matrix\n",
    "    classes : list, optional\n",
    "        List of class names\n",
    "    normalize : bool, default=False\n",
    "        Whether to normalize the confusion matrix\n",
    "    title : str, default='Confusion Matrix'\n",
    "        Title for the plot\n",
    "    cmap : matplotlib colormap, default=plt.cm.Blues\n",
    "        Colormap for the plot\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized Confusion Matrix\")\n",
    "    else:\n",
    "        print('Confusion Matrix, without normalization')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.colorbar()\n",
    "\n",
    "    if classes is None:\n",
    "        classes = ['Not Default', 'Default']\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], fmt),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True Label', fontsize=14)\n",
    "    plt.xlabel('Predicted Label', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred_probas, model_names):\n",
    "    \"\"\"\n",
    "    Plot ROC curves for multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred_probas : list of array-like\n",
    "        List of predicted probabilities for each model\n",
    "    model_names : list\n",
    "        List of model names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, (y_pred_proba, model_name) in enumerate(zip(y_pred_probas, model_names)):\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {auc:.4f})')\n",
    "    \n",
    "    # Plot random guessing line\n",
    "    plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--', label='Random Guessing')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=14)\n",
    "    plt.ylabel('True Positive Rate', fontsize=14)\n",
    "    plt.title('ROC Curve Comparison', fontsize=16)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_precision_recall_curve(y_true, y_pred_probas, model_names):\n",
    "    \"\"\"\n",
    "    Plot Precision-Recall curves for multiple models.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred_probas : list of array-like\n",
    "        List of predicted probabilities for each model\n",
    "    model_names : list\n",
    "        List of model names\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    for i, (y_pred_proba, model_name) in enumerate(zip(y_pred_probas, model_names)):\n",
    "        precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "        avg_precision = average_precision_score(y_true, y_pred_proba)\n",
    "        plt.plot(recall, precision, lw=2, label=f'{model_name} (AP = {avg_precision:.4f})')\n",
    "    \n",
    "    # Calculate baseline based on proportion of positive class\n",
    "    baseline = np.sum(y_true) / len(y_true)\n",
    "    plt.axhline(y=baseline, color='gray', linestyle='--', label=f'Baseline (AP = {baseline:.4f})')\n",
    "    \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Recall', fontsize=14)\n",
    "    plt.ylabel('Precision', fontsize=14)\n",
    "    plt.title('Precision-Recall Curve Comparison', fontsize=16)\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_feature_importance(feature_importance, feature_names, title=\"Feature Importance\", top_n=20):\n",
    "    \"\"\"\n",
    "    Plot feature importances.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_importance : array-like\n",
    "        Feature importance scores\n",
    "    feature_names : list\n",
    "        List of feature names\n",
    "    title : str, default=\"Feature Importance\"\n",
    "        Title for the plot\n",
    "    top_n : int, default=20\n",
    "        Number of top features to display\n",
    "    \"\"\"\n",
    "    # Create DataFrame for easier handling\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    # Select top N features\n",
    "    if len(importance_df) > top_n:\n",
    "        importance_df = importance_df.head(top_n)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df)\n",
    "    plt.title(title, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=14)\n",
    "    plt.ylabel('Feature', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "def compare_models(models_metrics, metric_names=None):\n",
    "    \"\"\"\n",
    "    Compare multiple models across various metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models_metrics : dict\n",
    "        Dictionary with model names as keys and metric dictionaries as values\n",
    "    metric_names : list, optional\n",
    "        List of metrics to compare (defaults to accuracy, precision, recall, f1, auc_roc)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        DataFrame comparing models across metrics\n",
    "    \"\"\"\n",
    "    if metric_names is None:\n",
    "        metric_names = ['accuracy', 'precision', 'recall', 'f1', 'auc_roc', 'avg_precision']\n",
    "    \n",
    "    # Create DataFrame for comparison\n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, metrics in models_metrics.items():\n",
    "        model_data = {'Model': model_name}\n",
    "        \n",
    "        for metric in metric_names:\n",
    "            if metric in metrics:\n",
    "                model_data[metric] = metrics[metric]\n",
    "            else:\n",
    "                model_data[metric] = None\n",
    "        \n",
    "        comparison_data.append(model_data)\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    # Plot comparison\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    \n",
    "    for i, metric in enumerate(metric_names):\n",
    "        if metric in ['confusion_matrix']:  # Skip non-numeric metrics\n",
    "            continue\n",
    "            \n",
    "        if all(comparison_df[metric].notna()):  # Only plot if all models have this metric\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            sns.barplot(x='Model', y=metric, data=comparison_df)\n",
    "            plt.title(f'Comparison by {metric.replace(\"_\", \" \").title()}')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.ylim(0, 1)  # Most metrics are between 0 and 1\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df.sort_values('f1', ascending=False)\n",
    "\n",
    "def threshold_optimization(y_true, y_pred_proba, thresholds=None, metric='f1'):\n",
    "    \"\"\"\n",
    "    Find optimal threshold for binary classification based on a specific metric.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred_proba : array-like\n",
    "        Predicted probabilities for the positive class\n",
    "    thresholds : array-like, optional\n",
    "        List of thresholds to evaluate (default: 100 values from 0.01 to 0.99)\n",
    "    metric : str, default='f1'\n",
    "        Metric to optimize ('accuracy', 'precision', 'recall', 'f1')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Optimal threshold and corresponding metric value\n",
    "    \"\"\"\n",
    "    if thresholds is None:\n",
    "        thresholds = np.linspace(0.01, 0.99, 100)\n",
    "    \n",
    "    best_threshold = 0.5  # Default threshold\n",
    "    best_metric_value = 0\n",
    "    \n",
    "    results = {'threshold': [], 'value': []}\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "        \n",
    "        if metric == 'accuracy':\n",
    "            value = accuracy_score(y_true, y_pred)\n",
    "        elif metric == 'precision':\n",
    "            value = precision_score(y_true, y_pred)\n",
    "        elif metric == 'recall':\n",
    "            value = recall_score(y_true, y_pred)\n",
    "        elif metric == 'f1':\n",
    "            value = f1_score(y_true, y_pred)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported metric: {metric}\")\n",
    "        \n",
    "        results['threshold'].append(threshold)\n",
    "        results['value'].append(value)\n",
    "        \n",
    "        if value > best_metric_value:\n",
    "            best_metric_value = value\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    # Plot threshold vs. metric\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(results['threshold'], results['value'])\n",
    "    plt.axvline(x=best_threshold, color='r', linestyle='--', \n",
    "                label=f'Best Threshold = {best_threshold:.2f}')\n",
    "    plt.axhline(y=best_metric_value, color='g', linestyle='--', \n",
    "                label=f'Best {metric} = {best_metric_value:.4f}')\n",
    "    plt.xlabel('Threshold', fontsize=14)\n",
    "    plt.ylabel(f'{metric.capitalize()} Score', fontsize=14)\n",
    "    plt.title(f'Threshold Optimization for {metric.capitalize()}', fontsize=16)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    return best_threshold, best_metric_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceabffac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed dataset from the feature engineering notebook\n",
    "selected_features_path = \"../data/processed/kenyan_loan_default_engineered_selected.csv\"\n",
    "df = pd.read_csv(selected_features_path)\n",
    "\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "df.head()\n",
    "\n",
    "# Check for any missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "if len(missing_values) > 0:\n",
    "    print(\"\\nColumns with missing values:\")\n",
    "    print(missing_values)\n",
    "else:\n",
    "    print(\"\\nNo missing values found in the dataset.\")\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"\\nBasic statistics of the dataset:\")\n",
    "df.describe().T\n",
    "\n",
    "# Check class distribution (target variable)\n",
    "target_counts = df['defaulted'].value_counts(normalize=True) * 100\n",
    "print(\"\\nClass distribution:\")\n",
    "print(target_counts)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='defaulted', data=df, palette=['#2ecc71', '#e74c3c'])\n",
    "plt.title('Class Distribution of Default Status', fontsize=16)\n",
    "plt.xlabel('Default Status (0 = No Default, 1 = Default)')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Add percentages to the bars\n",
    "for i, percentage in enumerate(target_counts):\n",
    "    count = df['defaulted'].value_counts()[i]\n",
    "    plt.text(i, count + 30, f'{percentage:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('defaulted', axis=1)\n",
    "y = df['defaulted']\n",
    "\n",
    "# Get feature names for later use\n",
    "feature_names = X.columns.tolist()\n",
    "\n",
    "print(f\"Number of features: {len(feature_names)}\")\n",
    "print(f\"Number of samples: {len(y)}\")\n",
    "print(f\"Class distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f0b3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets with stratification to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Testing set shape: {X_test.shape}\")\n",
    "print(f\"Training set class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Testing set class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ccd16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check imbalance ratio\n",
    "imbalance_ratio = np.bincount(y_train)[0] / np.bincount(y_train)[1]\n",
    "print(f\"Class imbalance ratio (majority:minority): {imbalance_ratio:.2f}:1\")\n",
    "\n",
    "# Function to apply resampling\n",
    "def apply_resampling(X, y, method='smote', sampling_strategy=0.5):\n",
    "    \"\"\"\n",
    "    Apply resampling to handle class imbalance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like\n",
    "        Feature matrix\n",
    "    y : array-like\n",
    "        Target vector\n",
    "    method : str, default='smote'\n",
    "        Resampling method: 'smote', 'adasyn', 'undersampling', 'smoteenn', 'smotetomek'\n",
    "    sampling_strategy : float or str, default=0.5\n",
    "        Ratio of minority to majority class after resampling\n",
    "        (if float), or 'auto'/'all' for specific strategies\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        Resampled feature matrix and target vector\n",
    "    \"\"\"\n",
    "    if method == 'smote':\n",
    "        resampler = SMOTE(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "    elif method == 'adasyn':\n",
    "        resampler = ADASYN(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "    elif method == 'undersampling':\n",
    "        resampler = RandomUnderSampler(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "    elif method == 'smoteenn':\n",
    "        resampler = SMOTEENN(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "    elif method == 'smotetomek':\n",
    "        resampler = SMOTETomek(sampling_strategy=sampling_strategy, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported resampling method: {method}\")\n",
    "    \n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "    \n",
    "    print(f\"Original shape: {X.shape}, Resampled shape: {X_resampled.shape}\")\n",
    "    print(f\"Original class distribution: {np.bincount(y)}\")\n",
    "    print(f\"Resampled class distribution: {np.bincount(y_resampled)}\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# Apply SMOTE to handle class imbalance in the training set\n",
    "X_train_resampled, y_train_resampled = apply_resampling(X_train, y_train, method='smote', sampling_strategy=0.8)\n",
    "\n",
    "# Visualize class distribution before and after resampling\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Before resampling\n",
    "sns.countplot(x=y_train, ax=ax1, palette=['#2ecc71', '#e74c3c'])\n",
    "ax1.set_title('Class Distribution Before Resampling')\n",
    "ax1.set_xlabel('Default Status')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# After resampling\n",
    "sns.countplot(x=y_train_resampled, ax=ax2, palette=['#2ecc71', '#e74c3c'])\n",
    "ax2.set_title('Class Distribution After Resampling')\n",
    "ax2.set_xlabel('Default Status')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5d082c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a logistic regression model as a baseline\n",
    "log_reg = LogisticRegression(random_state=RANDOM_STATE, max_iter=1000, \n",
    "                             class_weight='balanced', solver='liblinear')\n",
    "log_reg.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_log_reg = log_reg.predict(X_test)\n",
    "y_pred_proba_log_reg = log_reg.predict_proba(X_test)[:, 1]  # Probability for the positive class\n",
    "\n",
    "# Evaluate the model\n",
    "log_reg_metrics = evaluate_model(y_test, y_pred_log_reg, y_pred_proba_log_reg, \"Logistic Regression\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(log_reg_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Logistic Regression Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance (coefficients)\n",
    "if hasattr(log_reg, 'coef_'):\n",
    "    coef = log_reg.coef_[0]\n",
    "    log_reg_importance = np.abs(coef)  # Take absolute value for importance\n",
    "    log_reg_importance_df = plot_feature_importance(log_reg_importance, feature_names, \n",
    "                                                  \"Logistic Regression Feature Importance (Absolute Coefficients)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e468d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree model\n",
    "dt = DecisionTreeClassifier(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "dt.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_dt = dt.predict(X_test)\n",
    "y_pred_proba_dt = dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "dt_metrics = evaluate_model(y_test, y_pred_dt, y_pred_proba_dt, \"Decision Tree\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(dt_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Decision Tree Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(dt, 'feature_importances_'):\n",
    "    dt_importance_df = plot_feature_importance(dt.feature_importances_, feature_names, \n",
    "                                            \"Decision Tree Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99d8e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced')\n",
    "rf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "y_pred_proba_rf = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "rf_metrics = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(rf_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Random Forest Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(rf, 'feature_importances_'):\n",
    "    rf_importance_df = plot_feature_importance(rf.feature_importances_, feature_names, \n",
    "                                            \"Random Forest Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f6dfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a gradient boosting model\n",
    "gb = GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)\n",
    "gb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_gb = gb.predict(X_test)\n",
    "y_pred_proba_gb = gb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "gb_metrics = evaluate_model(y_test, y_pred_gb, y_pred_proba_gb, \"Gradient Boosting\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(gb_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Gradient Boosting Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(gb, 'feature_importances_'):\n",
    "    gb_importance_df = plot_feature_importance(gb.feature_importances_, feature_names, \n",
    "                                            \"Gradient Boosting Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a0a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the baseline models\n",
    "baseline_models_metrics = {\n",
    "    'Logistic Regression': log_reg_metrics,\n",
    "    'Decision Tree': dt_metrics,\n",
    "    'Random Forest': rf_metrics,\n",
    "    'Gradient Boosting': gb_metrics\n",
    "}\n",
    "\n",
    "baseline_comparison = compare_models(baseline_models_metrics)\n",
    "print(\"Baseline Models Comparison:\")\n",
    "print(baseline_comparison)\n",
    "\n",
    "# Plot ROC curves\n",
    "baseline_y_pred_probas = [y_pred_proba_log_reg, y_pred_proba_dt, \n",
    "                         y_pred_proba_rf, y_pred_proba_gb]\n",
    "baseline_model_names = ['Logistic Regression', 'Decision Tree', \n",
    "                       'Random Forest', 'Gradient Boosting']\n",
    "\n",
    "plot_roc_curve(y_test, baseline_y_pred_probas, baseline_model_names)\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plot_precision_recall_curve(y_test, baseline_y_pred_probas, baseline_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a1a140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train an XGBoost model\n",
    "xgb_model = xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "y_pred_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "xgb_metrics = evaluate_model(y_test, y_pred_xgb, y_pred_proba_xgb, \"XGBoost\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(xgb_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='XGBoost Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(xgb_model, 'feature_importances_'):\n",
    "    xgb_importance_df = plot_feature_importance(xgb_model.feature_importances_, feature_names, \n",
    "                                             \"XGBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d80366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a LightGBM model\n",
    "lgb_model = lgb.LGBMClassifier(random_state=RANDOM_STATE, class_weight='balanced')\n",
    "lgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lgb = lgb_model.predict(X_test)\n",
    "y_pred_proba_lgb = lgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "lgb_metrics = evaluate_model(y_test, y_pred_lgb, y_pred_proba_lgb, \"LightGBM\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(lgb_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='LightGBM Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(lgb_model, 'feature_importances_'):\n",
    "    lgb_importance_df = plot_feature_importance(lgb_model.feature_importances_, feature_names, \n",
    "                                             \"LightGBM Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2983b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a CatBoost model\n",
    "catboost_model = CatBoostClassifier(random_state=RANDOM_STATE, verbose=0, \n",
    "                                    class_weights={0: 1, 1: imbalance_ratio})\n",
    "catboost_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_catboost = catboost_model.predict(X_test)\n",
    "y_pred_proba_catboost = catboost_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "catboost_metrics = evaluate_model(y_test, y_pred_catboost, y_pred_proba_catboost, \"CatBoost\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(catboost_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='CatBoost Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(catboost_model, 'feature_importances_'):\n",
    "    catboost_importance_df = plot_feature_importance(catboost_model.feature_importances_, feature_names, \n",
    "                                                  \"CatBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c5b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the advanced models\n",
    "advanced_models_metrics = {\n",
    "    'XGBoost': xgb_metrics,\n",
    "    'LightGBM': lgb_metrics,\n",
    "    'CatBoost': catboost_metrics\n",
    "}\n",
    "\n",
    "advanced_comparison = compare_models(advanced_models_metrics)\n",
    "print(\"Advanced Models Comparison:\")\n",
    "print(advanced_comparison)\n",
    "\n",
    "# Plot ROC curves\n",
    "advanced_y_pred_probas = [y_pred_proba_xgb, y_pred_proba_lgb, y_pred_proba_catboost]\n",
    "advanced_model_names = ['XGBoost', 'LightGBM', 'CatBoost']\n",
    "\n",
    "plot_roc_curve(y_test, advanced_y_pred_probas, advanced_model_names)\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plot_precision_recall_curve(y_test, advanced_y_pred_probas, advanced_model_names)\n",
    "\n",
    "# Determine the best model for hyperparameter tuning\n",
    "all_models_metrics = {**baseline_models_metrics, **advanced_models_metrics}\n",
    "all_models_comparison = compare_models(all_models_metrics)\n",
    "print(\"\\nAll Models Comparison:\")\n",
    "print(all_models_comparison)\n",
    "\n",
    "# Select the best model for hyperparameter tuning\n",
    "best_model_name = all_models_comparison.iloc[0]['Model']\n",
    "print(f\"\\nBest model: {best_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for XGBoost\n",
    "# Define the parameter grid for RandomizedSearchCV\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 500],\n",
    "    'max_depth': [3, 4, 5, 6, 8, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'scale_pos_weight': [1, imbalance_ratio]  # Adjust for class imbalance\n",
    "}\n",
    "\n",
    "# Set up RandomizedSearchCV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "xgb_random_search = RandomizedSearchCV(\n",
    "    estimator=xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=RANDOM_STATE),\n",
    "    param_distributions=xgb_param_grid,\n",
    "    n_iter=50,  # Number of parameter settings sampled\n",
    "    scoring='f1',  # Optimize for F1 score\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "# Fit RandomizedSearchCV\n",
    "print(\"Starting hyperparameter tuning for XGBoost...\")\n",
    "start_time = time.time()\n",
    "xgb_random_search.fit(X_train, y_train)  # Use original training data, not resampled\n",
    "end_time = time.time()\n",
    "print(f\"Hyperparameter tuning completed in {(end_time - start_time) / 60:.2f} minutes\")\n",
    "\n",
    "# Get best parameters and best score\n",
    "print(f\"Best parameters: {xgb_random_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {xgb_random_search.best_score_:.4f}\")\n",
    "\n",
    "# Train the best model\n",
    "best_xgb_model = xgb.XGBClassifier(\n",
    "    **xgb_random_search.best_params_,\n",
    "    random_state=RANDOM_STATE,\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "best_xgb_model.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_best_xgb = best_xgb_model.predict(X_test)\n",
    "y_pred_proba_best_xgb = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the tuned model\n",
    "best_xgb_metrics = evaluate_model(y_test, y_pred_best_xgb, y_pred_proba_best_xgb, \"Tuned XGBoost\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(best_xgb_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Tuned XGBoost Confusion Matrix')\n",
    "\n",
    "# Analyze feature importance\n",
    "if hasattr(best_xgb_model, 'feature_importances_'):\n",
    "    best_xgb_importance_df = plot_feature_importance(best_xgb_model.feature_importances_, feature_names, \n",
    "                                                  \"Tuned XGBoost Feature Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3ae393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize the classification threshold for the best model\n",
    "best_threshold, best_f1 = threshold_optimization(\n",
    "    y_test, y_pred_proba_best_xgb, metric='f1'\n",
    ")\n",
    "\n",
    "print(f\"Optimal threshold: {best_threshold:.4f}, F1 score: {best_f1:.4f}\")\n",
    "\n",
    "# Apply the optimal threshold\n",
    "y_pred_best_threshold = (y_pred_proba_best_xgb >= best_threshold).astype(int)\n",
    "\n",
    "# Evaluate with the optimal threshold\n",
    "best_threshold_metrics = evaluate_model(\n",
    "    y_test, y_pred_best_threshold, y_pred_proba_best_xgb, \n",
    "    f\"Tuned XGBoost (Threshold = {best_threshold:.2f})\"\n",
    ")\n",
    "\n",
    "# Plot confusion matrix with the optimal threshold\n",
    "plot_confusion_matrix(\n",
    "    best_threshold_metrics['confusion_matrix'], \n",
    "    classes=['Not Default', 'Default'], \n",
    "    title=f'Tuned XGBoost Confusion Matrix (Threshold = {best_threshold:.2f})'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52129b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a voting classifier with the best models\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced')),\n",
    "        ('xgb', xgb.XGBClassifier(**xgb_random_search.best_params_, random_state=RANDOM_STATE, \n",
    "                                 use_label_encoder=False, eval_metric='logloss')),\n",
    "        ('lgb', lgb.LGBMClassifier(random_state=RANDOM_STATE, class_weight='balanced')),\n",
    "    ],\n",
    "    voting='soft'  # Use predicted probabilities for voting\n",
    ")\n",
    "\n",
    "# Train the voting classifier\n",
    "voting_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_voting = voting_clf.predict(X_test)\n",
    "y_pred_proba_voting = voting_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "voting_metrics = evaluate_model(y_test, y_pred_voting, y_pred_proba_voting, \"Voting Classifier\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(voting_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Voting Classifier Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674f9f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a stacking classifier\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, class_weight='balanced')),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=RANDOM_STATE)),\n",
    "    ('xgb', xgb.XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'))\n",
    "]\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=LogisticRegression(random_state=RANDOM_STATE, class_weight='balanced'),\n",
    "    cv=5,\n",
    "    stack_method='predict_proba'\n",
    ")\n",
    "\n",
    "# Train the stacking classifier\n",
    "stacking_clf.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_stacking = stacking_clf.predict(X_test)\n",
    "y_pred_proba_stacking = stacking_clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Evaluate the model\n",
    "stacking_metrics = evaluate_model(y_test, y_pred_stacking, y_pred_proba_stacking, \"Stacking Classifier\")\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(stacking_metrics['confusion_matrix'], \n",
    "                      classes=['Not Default', 'Default'], \n",
    "                      title='Stacking Classifier Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3e57b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ensemble models to the comparison\n",
    "ensemble_models_metrics = {\n",
    "    'Voting Classifier': voting_metrics,\n",
    "    'Stacking Classifier': stacking_metrics,\n",
    "    'Tuned XGBoost': best_xgb_metrics,\n",
    "    'Tuned XGBoost (Optimal Threshold)': best_threshold_metrics\n",
    "}\n",
    "\n",
    "# Compare all models\n",
    "final_models_metrics = {**all_models_metrics, **ensemble_models_metrics}\n",
    "final_comparison = compare_models(final_models_metrics)\n",
    "print(\"Final Models Comparison:\")\n",
    "print(final_comparison)\n",
    "\n",
    "# Plot ROC curves for the final selection of models\n",
    "final_y_pred_probas = [y_pred_proba_best_xgb, y_pred_proba_voting, y_pred_proba_stacking]\n",
    "final_model_names = ['Tuned XGBoost', 'Voting Classifier', 'Stacking Classifier']\n",
    "\n",
    "plot_roc_curve(y_test, final_y_pred_probas, final_model_names)\n",
    "\n",
    "# Plot Precision-Recall curves\n",
    "plot_precision_recall_curve(y_test, final_y_pred_probas, final_model_names)\n",
    "\n",
    "# Determine the best overall model\n",
    "best_overall_model_name = final_comparison.iloc[0]['Model']\n",
    "print(f\"\\nBest overall model: {best_overall_model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957a2e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a small subset of the test data for SHAP analysis to keep visualization clear\n",
    "X_test_sample = X_test.sample(min(100, len(X_test)), random_state=RANDOM_STATE)\n",
    "\n",
    "# Initialize the SHAP explainer\n",
    "explainer = shap.Explainer(best_xgb_model)\n",
    "shap_values = explainer(X_test_sample)\n",
    "\n",
    "# Summary plot\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_sample, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Feature Importance\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed summary plot showing the impact of features\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(shap_values, X_test_sample, show=False)\n",
    "plt.title(\"SHAP Summary Plot\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Dependence plots for top features\n",
    "top_features = best_xgb_importance_df['Feature'].head(3).tolist()\n",
    "for feature in top_features:\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    shap.dependence_plot(feature, shap_values.values, X_test_sample, show=False)\n",
    "    plt.title(f\"SHAP Dependence Plot for {feature}\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46df12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create partial dependence plots for top features\n",
    "top_features = best_xgb_importance_df['Feature'].head(5).tolist()\n",
    "\n",
    "# Set up the figure\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, feature in enumerate(top_features):\n",
    "    if i < len(axes):\n",
    "        # Create PDPs\n",
    "        pdp_iso = pdp.pdp_isolate(\n",
    "            model=best_xgb_model,\n",
    "            dataset=X_test,\n",
    "            model_features=feature_names,\n",
    "            feature=feature\n",
    "        )\n",
    "        \n",
    "        # Plot PDPs\n",
    "        pdp.pdp_plot(pdp_iso, feature, ax=axes[i], center=True)\n",
    "        axes[i].set_title(f'PDP for {feature}', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Partial Dependence Plots for Top Features', fontsize=18, y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71baef97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze interactions between top features\n",
    "top_2_features = best_xgb_importance_df['Feature'].head(2).tolist()\n",
    "\n",
    "# Create interaction plot\n",
    "if len(top_2_features) >= 2:\n",
    "    pdp_interact = pdp.pdp_interact(\n",
    "        model=best_xgb_model,\n",
    "        dataset=X_test,\n",
    "        model_features=feature_names,\n",
    "        features=top_2_features\n",
    "    )\n",
    "    \n",
    "    # Plot the interaction\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "    pdp.pdp_interact_plot(\n",
    "        pdp_interact=pdp_interact,\n",
    "        feature_names=top_2_features,\n",
    "        plot_type='contour',\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    pdp.pdp_interact_plot(\n",
    "        pdp_interact=pdp_interact,\n",
    "        feature_names=top_2_features,\n",
    "        plot_type='grid',\n",
    "        ax=axes[1]\n",
    "    )\n",
    "    \n",
    "    plt.suptitle(f'Feature Interaction: {top_2_features[0]} and {top_2_features[1]}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f035f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation on the best model\n",
    "# We'll use the original non-resampled data for proper validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Define metrics to evaluate\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1',\n",
    "    'roc_auc': 'roc_auc'\n",
    "}\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_results = cross_val_score(best_xgb_model, X, y, cv=cv, scoring='f1')\n",
    "\n",
    "print(f\"Cross-validation F1 scores: {cv_results}\")\n",
    "print(f\"Mean F1 score: {cv_results.mean():.4f}\")\n",
    "print(f\"Standard deviation: {cv_results.std():.4f}\")\n",
    "\n",
    "# Visualize cross-validation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(cv_results) + 1), cv_results)\n",
    "plt.axhline(y=cv_results.mean(), color='r', linestyle='--', \n",
    "           label=f'Mean F1: {cv_results.mean():.4f}')\n",
    "plt.xlabel('Fold', fontsize=14)\n",
    "plt.ylabel('F1 Score', fontsize=14)\n",
    "plt.title('Cross-Validation F1 Scores', fontsize=16)\n",
    "plt.xticks(range(1, len(cv_results) + 1))\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define costs and benefits for business impact analysis\n",
    "# These values would be determined by domain experts and business stakeholders\n",
    "# For example:\n",
    "# - Cost of false positive (predicting default when there is none): Lost interest income\n",
    "# - Cost of false negative (missing a default): Lost principal and collection costs\n",
    "\n",
    "# Example values (in KES)\n",
    "cost_false_positive = 10000  # Lost interest income from rejecting a good loan\n",
    "cost_false_negative = 100000  # Lost principal from approving a bad loan\n",
    "\n",
    "# Calculate confusion matrix elements from the best model\n",
    "best_model_cm = best_threshold_metrics['confusion_matrix']\n",
    "tn, fp, fn, tp = best_model_cm.ravel()\n",
    "\n",
    "# Calculate total cost\n",
    "total_cost = (fp * cost_false_positive) + (fn * cost_false_negative)\n",
    "print(f\"Business Costs:\")\n",
    "print(f\"False Positives (Rejected Good Loans): {fp} × {cost_false_positive} KES = {fp * cost_false_positive} KES\")\n",
    "print(f\"False Negatives (Approved Bad Loans): {fn} × {cost_false_negative} KES = {fn * cost_false_negative} KES\")\n",
    "print(f\"Total Cost: {total_cost} KES\")\n",
    "\n",
    "# Calculate savings compared to a baseline (e.g., approving all loans)\n",
    "baseline_fn = np.sum(y_test)  # All defaults would be missed\n",
    "baseline_cost = baseline_fn * cost_false_negative\n",
    "savings = baseline_cost - total_cost\n",
    "print(f\"\\nBaseline Cost (Approving All Loans): {baseline_cost} KES\")\n",
    "print(f\"Cost with Model: {total_cost} KES\")\n",
    "print(f\"Savings: {savings} KES ({savings/baseline_cost*100:.2f}%)\")\n",
    "\n",
    "# Create a visual representation of costs\n",
    "labels = ['False Positives', 'False Negatives', 'Total Cost', 'Baseline Cost', 'Savings']\n",
    "values = [\n",
    "    fp * cost_false_positive, \n",
    "    fn * cost_false_negative, \n",
    "    total_cost, \n",
    "    baseline_cost, \n",
    "    savings\n",
    "]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(labels, values, color=['orange', 'red', 'purple', 'gray', 'green'])\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "             f'{height:,.0f} KES', ha='center', va='bottom', fontsize=12)\n",
    "\n",
    "plt.title('Business Impact Analysis', fontsize=16)\n",
    "plt.ylabel('Cost in KES', fontsize=14)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd1b0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk segments based on predicted probabilities\n",
    "# Define risk segments\n",
    "risk_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "risk_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "\n",
    "# Calculate actual default rates within each predicted risk segment\n",
    "risk_df = pd.DataFrame({\n",
    "    'predicted_proba': y_pred_proba_best_xgb,\n",
    "    'actual': y_test.values\n",
    "})\n",
    "\n",
    "risk_df['risk_segment'] = pd.cut(risk_df['predicted_proba'], bins=risk_bins, labels=risk_labels)\n",
    "\n",
    "# Calculate default rate by risk segment\n",
    "risk_analysis = risk_df.groupby('risk_segment').agg(\n",
    "    count=('actual', 'count'),\n",
    "    default_rate=('actual', 'mean'),\n",
    "    default_count=('actual', 'sum')\n",
    ")\n",
    "\n",
    "# Calculate the percentage of the portfolio in each segment\n",
    "risk_analysis['portfolio_percentage'] = risk_analysis['count'] / len(risk_df) * 100\n",
    "\n",
    "print(\"Risk Segmentation Analysis:\")\n",
    "print(risk_analysis)\n",
    "\n",
    "# Visualize risk segments\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# Default rate by risk segment\n",
    "sns.barplot(x=risk_analysis.index, y=risk_analysis['default_rate'], ax=ax1, palette='YlOrRd')\n",
    "ax1.set_title('Default Rate by Risk Segment', fontsize=16)\n",
    "ax1.set_xlabel('Risk Segment', fontsize=14)\n",
    "ax1.set_ylabel('Default Rate', fontsize=14)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(risk_analysis['default_rate']):\n",
    "    ax1.text(i, v + 0.02, f'{v:.1%}', ha='center', fontsize=12)\n",
    "\n",
    "# Portfolio distribution by risk segment\n",
    "sns.barplot(x=risk_analysis.index, y=risk_analysis['portfolio_percentage'], ax=ax2, palette='Blues')\n",
    "ax2.set_title('Portfolio Distribution by Risk Segment', fontsize=16)\n",
    "ax2.set_xlabel('Risk Segment', fontsize=14)\n",
    "ax2.set_ylabel('Percentage of Portfolio', fontsize=14)\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "for i, v in enumerate(risk_analysis['portfolio_percentage']):\n",
    "    ax2.text(i, v + 1, f'{v:.1f}%', ha='center', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a more detailed visualization of default distribution\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.histplot(data=risk_df, x='predicted_proba', hue='actual', bins=20, \n",
    "             kde=True, stat='probability', common_norm=False)\n",
    "plt.axvline(x=best_threshold, color='red', linestyle='--', \n",
    "           label=f'Optimal Threshold: {best_threshold:.2f}')\n",
    "plt.title('Distribution of Default Probability Scores', fontsize=16)\n",
    "plt.xlabel('Predicted Default Probability', fontsize=14)\n",
    "plt.ylabel('Probability Density', fontsize=14)\n",
    "plt.legend(['Non-Default', 'Default', 'Optimal Threshold'])\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973396b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our analysis, select the final model\n",
    "# Assuming the tuned XGBoost with optimal threshold is the best model\n",
    "final_model = best_xgb_model\n",
    "\n",
    "# Create a directory for model artifacts if it doesn't exist\n",
    "model_dir = \"../models\"\n",
    "Path(model_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save the model\n",
    "model_filename = f\"{model_dir}/loan_default_prediction_model.pkl\"\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"Final model saved to {model_filename}\")\n",
    "\n",
    "# Save the optimal threshold\n",
    "threshold_filename = f\"{model_dir}/optimal_threshold.pkl\"\n",
    "with open(threshold_filename, 'wb') as f:\n",
    "    pickle.dump(best_threshold, f)\n",
    "print(f\"Optimal threshold saved to {threshold_filename}\")\n",
    "\n",
    "# Save feature names\n",
    "feature_names_filename = f\"{model_dir}/feature_names.pkl\"\n",
    "with open(feature_names_filename, 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"Feature names saved to {feature_names_filename}\")\n",
    "\n",
    "# Create a model metadata file with performance metrics and training info\n",
    "model_metadata = {\n",
    "    'model_type': type(final_model).__name__,\n",
    "    'training_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'performance_metrics': {\n",
    "        'accuracy': best_threshold_metrics['accuracy'],\n",
    "        'precision': best_threshold_metrics['precision'],\n",
    "        'recall': best_threshold_metrics['recall'],\n",
    "        'f1': best_threshold_metrics['f1'],\n",
    "        'auc_roc': best_threshold_metrics['auc_roc'],\n",
    "        'avg_precision': best_threshold_metrics['avg_precision']\n",
    "    },\n",
    "    'optimal_threshold': best_threshold,\n",
    "    'feature_count': len(feature_names),\n",
    "    'class_distribution': {\n",
    "        'train_positive': np.sum(y_train),\n",
    "        'train_negative': len(y_train) - np.sum(y_train),\n",
    "        'test_positive': np.sum(y_test),\n",
    "        'test_negative': len(y_test) - np.sum(y_test)\n",
    "    },\n",
    "    'top_features': best_xgb_importance_df['Feature'].head(10).tolist()\n",
    "}\n",
    "\n",
    "# Save model metadata\n",
    "metadata_filename = f\"{model_dir}/model_metadata.json\"\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    import json\n",
    "    json.dump(model_metadata, f, indent=4)\n",
    "print(f\"Model metadata saved to {metadata_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477c00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction function that can be used in production\n",
    "def predict_loan_default(data, model=None, threshold=0.5, feature_names=None):\n",
    "    \"\"\"\n",
    "    Predict loan default probability and binary outcome.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pandas DataFrame\n",
    "        Data containing the features needed for prediction\n",
    "    model : trained model, optional\n",
    "        The trained model. If None, will load the saved model.\n",
    "    threshold : float, default=0.5\n",
    "        Classification threshold\n",
    "    feature_names : list, optional\n",
    "        List of feature names. If None, will load from saved file.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing probabilities and binary predictions\n",
    "    \"\"\"\n",
    "    # Load the model if not provided\n",
    "    if model is None:\n",
    "        model = joblib.load(f\"{model_dir}/loan_default_prediction_model.pkl\")\n",
    "    \n",
    "    # Load the optimal threshold if not provided\n",
    "    if threshold == 0.5:\n",
    "        with open(f\"{model_dir}/optimal_threshold.pkl\", 'rb') as f:\n",
    "            threshold = pickle.load(f)\n",
    "    \n",
    "    # Load feature names if not provided\n",
    "    if feature_names is None:\n",
    "        with open(f\"{model_dir}/feature_names.pkl\", 'rb') as f:\n",
    "            feature_names = pickle.load(f)\n",
    "    \n",
    "    # Ensure data has all required features\n",
    "    missing_features = set(feature_names) - set(data.columns)\n",
    "    if missing_features:\n",
    "        raise ValueError(f\"Missing features in input data: {missing_features}\")\n",
    "    \n",
    "    # Select and order features correctly\n",
    "    X = data[feature_names]\n",
    "    \n",
    "    # Make predictions\n",
    "    probabilities = model.predict_proba(X)[:, 1]\n",
    "    predictions = (probabilities >= threshold).astype(int)\n",
    "    \n",
    "    # Create risk segments\n",
    "    risk_bins = [0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "    risk_labels = ['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    "    risk_segments = pd.cut(probabilities, bins=risk_bins, labels=risk_labels).astype(str)\n",
    "    \n",
    "    # Return results\n",
    "    return {\n",
    "        'probabilities': probabilities,\n",
    "        'predictions': predictions,\n",
    "        'risk_segments': risk_segments\n",
    "    }\n",
    "\n",
    "# Test the prediction function on a sample of test data\n",
    "test_sample = X_test.head(5)\n",
    "prediction_results = predict_loan_default(\n",
    "    test_sample, model=final_model, threshold=best_threshold, feature_names=feature_names\n",
    ")\n",
    "\n",
    "print(\"Sample Prediction Results:\")\n",
    "for i in range(len(test_sample)):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"  Default Probability: {prediction_results['probabilities'][i]:.4f}\")\n",
    "    print(f\"  Prediction: {'Default' if prediction_results['predictions'][i] == 1 else 'No Default'}\")\n",
    "    print(f\"  Risk Segment: {prediction_results['risk_segments'][i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039908ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
