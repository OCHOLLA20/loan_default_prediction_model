{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9b0744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, chi2, mutual_info_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "%matplotlib inline\n",
    "\n",
    "# Display all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define path to the dataset\n",
    "file_path = \"../data/raw/kenyan_loan_default_dataset.csv\"\n",
    "\n",
    "# Load the raw dataset\n",
    "df_raw = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows\n",
    "print(f\"Dataset shape: {df_raw.shape}\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fec1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create a copy of the raw data to work with\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Create a DataFrame to display missing values and percentages\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Values': missing_values,\n",
    "    'Percentage (%)': missing_percentage\n",
    "})\n",
    "\n",
    "# Display only columns with missing values, sorted by percentage\n",
    "missing_df = missing_df[missing_df['Missing Values'] > 0].sort_values('Percentage (%)', ascending=False)\n",
    "print(\"Columns with Missing Values:\")\n",
    "missing_df\n",
    "\n",
    "# Handle missing values based on column type\n",
    "# For numerical columns, we'll fill with median\n",
    "numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_value = df[col].median()\n",
    "        df[col] = df[col].fillna(median_value)\n",
    "        print(f\"Filled missing values in {col} with median: {median_value}\")\n",
    "\n",
    "# For categorical columns, we'll fill with mode\n",
    "categorical_cols = df.select_dtypes(include=['object', 'bool']).columns\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode_value = df[col].mode()[0]\n",
    "        df[col] = df[col].fillna(mode_value)\n",
    "        print(f\"Filled missing values in {col} with mode: {mode_value}\")\n",
    "\n",
    "# Verify all missing values are handled\n",
    "print(\"\\nRemaining missing values:\")\n",
    "print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58189a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect and handle outliers using IQR method\n",
    "def handle_outliers(df, column, method='cap'):\n",
    "    \"\"\"\n",
    "    Detect and handle outliers in a column.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame containing the column\n",
    "    column : str\n",
    "        The column name to process\n",
    "    method : str, default='cap'\n",
    "        The method to handle outliers ('cap' or 'remove')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame with outliers handled\n",
    "    \"\"\"\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers\n",
    "    outliers = df[(df[column] < lower_bound) | (df[column] > upper_bound)][column].count()\n",
    "    outliers_percent = (outliers / len(df)) * 100\n",
    "    print(f\"{column}: {outliers} outliers detected ({outliers_percent:.2f}%)\")\n",
    "    \n",
    "    if method == 'cap':\n",
    "        # Cap the outliers instead of removing\n",
    "        df[column] = np.where(df[column] < lower_bound, lower_bound, df[column])\n",
    "        df[column] = np.where(df[column] > upper_bound, upper_bound, df[column])\n",
    "    elif method == 'remove':\n",
    "        # Remove outliers (be careful with this approach)\n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply outlier handling to key numerical columns\n",
    "numerical_columns_for_outlier_treatment = [\n",
    "    'age', 'monthly_income_kes', 'mobile_money_usage', 'loan_amount_kes', \n",
    "    'interest_rate', 'days_late', 'airtime_topup_frequency'\n",
    "]\n",
    "\n",
    "for col in numerical_columns_for_outlier_treatment:\n",
    "    df = handle_outliers(df, col, method='cap')\n",
    "\n",
    "# Let's visualize the distributions after outlier treatment\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, col in enumerate(numerical_columns_for_outlier_treatment):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.histplot(df[col], kde=True)\n",
    "    plt.title(f'Distribution of {col} (after outlier treatment)')\n",
    "plt.tight_layout()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6615e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's identify all categorical columns\n",
    "categorical_columns = df.select_dtypes(include=['object', 'bool']).columns.tolist()\n",
    "print(f\"Categorical columns to encode: {categorical_columns}\")\n",
    "\n",
    "# Make a copy of the dataframe before encoding\n",
    "df_encoded = df.copy()\n",
    "\n",
    "# Encode binary categorical variables\n",
    "binary_cols = ['group_loan']\n",
    "for col in binary_cols:\n",
    "    if col in df_encoded.columns and df_encoded[col].dtype == 'object':\n",
    "        df_encoded[col] = df_encoded[col].map({'Yes': 1, 'No': 0})\n",
    "        print(f\"Binary encoded {col}\")\n",
    "\n",
    "# One-hot encoding for categorical variables with multiple categories\n",
    "# We'll exclude 'loan_id' and 'customer_id' as they are identifiers\n",
    "categorical_to_encode = [col for col in categorical_columns if col not in ['loan_id', 'customer_id', 'group_loan']]\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=categorical_to_encode, drop_first=True)\n",
    "\n",
    "# Display the encoded dataframe\n",
    "print(f\"\\nShape after encoding: {df_encoded.shape}\")\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094feb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create financial ratio features based on EDA insights\n",
    "\n",
    "# Loan-to-Income Ratio (higher ratio might indicate higher risk)\n",
    "df_encoded['loan_to_income_ratio'] = df_encoded['loan_amount_kes'] / df_encoded['monthly_income_kes']\n",
    "\n",
    "# Monthly payment estimate (simplified calculation)\n",
    "df_encoded['estimated_monthly_payment'] = (df_encoded['loan_amount_kes'] * \n",
    "                                         (1 + df_encoded['interest_rate']/100)) / df_encoded['loan_term_months']\n",
    "\n",
    "# Debt-to-Income Ratio (estimated monthly payment to monthly income)\n",
    "df_encoded['debt_to_income_ratio'] = df_encoded['estimated_monthly_payment'] / df_encoded['monthly_income_kes']\n",
    "\n",
    "# Mobile Money to Income Ratio\n",
    "df_encoded['mobile_money_to_income_ratio'] = df_encoded['mobile_money_usage'] / df_encoded['monthly_income_kes']\n",
    "\n",
    "# Repayment Rate (repayment progress relative to loan term)\n",
    "df_encoded['repayment_rate'] = df_encoded['repayment_progress'] / (df_encoded['days_late'] + 1)\n",
    "\n",
    "# Interest Burden (total interest as a percentage of loan)\n",
    "df_encoded['interest_burden'] = (df_encoded['loan_amount_kes'] * df_encoded['interest_rate'] / 100 * \n",
    "                                df_encoded['loan_term_months'] / 12) / df_encoded['loan_amount_kes']\n",
    "\n",
    "# Risk Factor (weighted combination of key risk indicators)\n",
    "df_encoded['risk_factor'] = (0.3 * df_encoded['loan_to_income_ratio'] + \n",
    "                           0.3 * df_encoded['num_defaults'] + \n",
    "                           0.2 * (df_encoded['days_late'] / 30) +  # Normalize to months\n",
    "                           0.2 * (1 - df_encoded['repayment_progress']/100))  # Convert to 0-1 scale\n",
    "\n",
    "# Display summary of new ratio features\n",
    "ratio_features = ['loan_to_income_ratio', 'estimated_monthly_payment', 'debt_to_income_ratio', \n",
    "                 'mobile_money_to_income_ratio', 'repayment_rate', 'interest_burden', 'risk_factor']\n",
    "\n",
    "print(\"Summary statistics for new ratio features:\")\n",
    "df_encoded[ratio_features].describe().T\n",
    "\n",
    "# Visualize the relationship between ratios and default status\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feat in enumerate(ratio_features):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    sns.boxplot(x='defaulted', y=feat, data=df_encoded)\n",
    "    plt.title(f'{feat} by Default Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858446c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features related to mobile money usage and financial behavior\n",
    "\n",
    "# Mobile Money Activity Level (categorical based on usage)\n",
    "df_encoded['mobile_money_activity'] = pd.qcut(\n",
    "    df_encoded['mobile_money_usage'], \n",
    "    q=5, \n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "# Mobile Money to Airtime Ratio\n",
    "df_encoded['mobile_money_to_airtime_ratio'] = df_encoded['mobile_money_usage'] / (df_encoded['airtime_topup_frequency'] + 1)\n",
    "\n",
    "# Financial Activity Index (combined measure of mobile and airtime usage)\n",
    "df_encoded['financial_activity_index'] = (\n",
    "    (df_encoded['mobile_money_usage'] / df_encoded['mobile_money_usage'].max()) * 0.7 + \n",
    "    (df_encoded['airtime_topup_frequency'] / df_encoded['airtime_topup_frequency'].max()) * 0.3\n",
    ")\n",
    "\n",
    "# Convert 'mobile_money_activity' back to numerical for modeling\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['mobile_money_activity'], prefix='mm_activity')\n",
    "\n",
    "# Display the new behavioral features\n",
    "behavioral_features = ['mobile_money_to_airtime_ratio', 'financial_activity_index'] + [col for col in df_encoded.columns if 'mm_activity_' in col]\n",
    "print(\"Summary of new behavioral features:\")\n",
    "df_encoded[behavioral_features[:2]].describe().T\n",
    "\n",
    "# Visualize the relationship with default status\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feat in enumerate(behavioral_features[:2]):  # Plot just the continuous features\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    sns.boxplot(x='defaulted', y=feat, data=df_encoded)\n",
    "    plt.title(f'{feat} by Default Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf9210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features based on loan history and borrower characteristics\n",
    "\n",
    "# Default Rate (ratio of defaults to previous loans)\n",
    "df_encoded['default_rate'] = df_encoded['num_defaults'] / (df_encoded['num_previous_loans'] + 1)  # Add 1 to avoid division by zero\n",
    "\n",
    "# Loan Experience Level\n",
    "df_encoded['loan_experience'] = pd.cut(\n",
    "    df_encoded['num_previous_loans'],\n",
    "    bins=[-1, 0, 2, 5, 10, float('inf')],\n",
    "    labels=['First-time', 'Beginner', 'Intermediate', 'Experienced', 'Expert']\n",
    ")\n",
    "\n",
    "# Default History Severity\n",
    "df_encoded['default_history_severity'] = pd.cut(\n",
    "    df_encoded['default_rate'],\n",
    "    bins=[-0.001, 0.001, 0.2, 0.5, 1.0],  # -0.001 to include 0\n",
    "    labels=['Clean', 'Low Risk', 'Medium Risk', 'High Risk']\n",
    ")\n",
    "\n",
    "# Current Loan Progress Index\n",
    "df_encoded['loan_progress_index'] = df_encoded['repayment_progress'] / (df_encoded['days_late'] + 1)\n",
    "\n",
    "# Convert categorical features to dummy variables\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['loan_experience', 'default_history_severity'], prefix=['loan_exp', 'default_hist'])\n",
    "\n",
    "# Display the new history-based features\n",
    "history_features = ['default_rate', 'loan_progress_index'] + [col for col in df_encoded.columns if 'loan_exp_' in col or 'default_hist_' in col]\n",
    "print(\"Summary of new history-based features:\")\n",
    "df_encoded[history_features[:2]].describe().T\n",
    "\n",
    "# Visualize the relationship with default status\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i, feat in enumerate(history_features[:2]):  # Plot just the continuous features\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    sns.boxplot(x='defaulted', y=feat, data=df_encoded)\n",
    "    plt.title(f'{feat} by Default Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856c6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features based on demographic and socioeconomic characteristics\n",
    "\n",
    "# Age Groups (more granular than typical binning)\n",
    "df_encoded['age_group'] = pd.cut(\n",
    "    df_encoded['age'],\n",
    "    bins=[18, 25, 30, 35, 40, 45, 50, 60, 100],\n",
    "    labels=['18-25', '26-30', '31-35', '36-40', '41-45', '46-50', '51-60', '60+']\n",
    ")\n",
    "\n",
    "# Income Level\n",
    "df_encoded['income_level'] = pd.qcut(\n",
    "    df_encoded['monthly_income_kes'],\n",
    "    q=5,\n",
    "    labels=['Very Low', 'Low', 'Medium', 'High', 'Very High']\n",
    ")\n",
    "\n",
    "# Socioeconomic Score (combined measure of income, education, and employment)\n",
    "# First, we need to identify education and employment columns from one-hot encoding\n",
    "education_cols = [col for col in df_encoded.columns if 'education_level_' in col]\n",
    "employment_cols = [col for col in df_encoded.columns if 'employment_status_' in col]\n",
    "\n",
    "# Create an education score (simple method: weighting different levels)\n",
    "if education_cols:\n",
    "    # Example weights based on education level\n",
    "    education_weights = {\n",
    "        'education_level_Tertiary': 3,\n",
    "        'education_level_Secondary': 2,\n",
    "        'education_level_Primary': 1,\n",
    "        'education_level_None': 0\n",
    "    }\n",
    "    \n",
    "    # Initialize education score\n",
    "    df_encoded['education_score'] = 0\n",
    "    \n",
    "    # Apply weights where columns exist\n",
    "    for col, weight in education_weights.items():\n",
    "        if col in df_encoded.columns:\n",
    "            df_encoded['education_score'] += df_encoded[col] * weight\n",
    "\n",
    "# Employment stability score\n",
    "if employment_cols:\n",
    "    # Example weights based on employment stability\n",
    "    employment_weights = {\n",
    "        'employment_status_Formal': 3,\n",
    "        'employment_status_Self-employed': 2,\n",
    "        'employment_status_Informal': 1,\n",
    "        'employment_status_Unemployed': 0\n",
    "    }\n",
    "    \n",
    "    # Initialize employment score\n",
    "    df_encoded['employment_score'] = 0\n",
    "    \n",
    "    # Apply weights where columns exist\n",
    "    for col, weight in employment_weights.items():\n",
    "        if col in df_encoded.columns:\n",
    "            df_encoded['employment_score'] += df_encoded[col] * weight\n",
    "\n",
    "# Normalize income for socioeconomic score\n",
    "df_encoded['income_score'] = (df_encoded['monthly_income_kes'] - df_encoded['monthly_income_kes'].min()) / (df_encoded['monthly_income_kes'].max() - df_encoded['monthly_income_kes'].min())\n",
    "\n",
    "# Calculate socioeconomic score (weighted combination)\n",
    "df_encoded['socioeconomic_score'] = (\n",
    "    df_encoded['income_score'] * 0.5 + \n",
    "    df_encoded['education_score'] / 3 * 0.3 +  # Normalize to 0-1\n",
    "    df_encoded['employment_score'] / 3 * 0.2    # Normalize to 0-1\n",
    ")\n",
    "\n",
    "# Convert categorical features to dummy variables\n",
    "df_encoded = pd.get_dummies(df_encoded, columns=['age_group', 'income_level'], prefix=['age', 'income'])\n",
    "\n",
    "# Display the new demographic features\n",
    "demographic_features = ['education_score', 'employment_score', 'income_score', 'socioeconomic_score']\n",
    "print(\"Summary of new demographic features:\")\n",
    "df_encoded[demographic_features].describe().T\n",
    "\n",
    "# Visualize the relationship with default status\n",
    "plt.figure(figsize=(15, 8))\n",
    "for i, feat in enumerate(demographic_features):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    sns.boxplot(x='defaulted', y=feat, data=df_encoded)\n",
    "    plt.title(f'{feat} by Default Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195f6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interaction features between key variables\n",
    "\n",
    "# Interaction between loan amount and interest rate\n",
    "df_encoded['loan_amount_interest_interaction'] = df_encoded['loan_amount_kes'] * df_encoded['interest_rate']\n",
    "\n",
    "# Interaction between income and previous defaults\n",
    "df_encoded['income_defaults_interaction'] = df_encoded['monthly_income_kes'] * (1 / (df_encoded['num_defaults'] + 1))\n",
    "\n",
    "# Interaction between mobile money usage and loan amount\n",
    "df_encoded['mobile_money_loan_interaction'] = df_encoded['mobile_money_usage'] / (df_encoded['loan_amount_kes'] + 1)\n",
    "\n",
    "# Interaction between loan term and interest rate\n",
    "df_encoded['term_interest_interaction'] = df_encoded['loan_term_months'] * df_encoded['interest_rate']\n",
    "\n",
    "# Interaction between age and loan amount\n",
    "df_encoded['age_loan_interaction'] = df_encoded['age'] * df_encoded['loan_amount_kes']\n",
    "\n",
    "# Interaction between socioeconomic score and loan-to-income ratio\n",
    "df_encoded['socio_lti_interaction'] = df_encoded['socioeconomic_score'] * df_encoded['loan_to_income_ratio']\n",
    "\n",
    "# Display the new interaction features\n",
    "interaction_features = [\n",
    "    'loan_amount_interest_interaction', 'income_defaults_interaction', \n",
    "    'mobile_money_loan_interaction', 'term_interest_interaction',\n",
    "    'age_loan_interaction', 'socio_lti_interaction'\n",
    "]\n",
    "print(\"Summary of new interaction features:\")\n",
    "df_encoded[interaction_features].describe().T\n",
    "\n",
    "# Visualize the relationship with default status\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, feat in enumerate(interaction_features):\n",
    "    plt.subplot(3, 2, i+1)\n",
    "    sns.boxplot(x='defaulted', y=feat, data=df_encoded)\n",
    "    plt.title(f'{feat} by Default Status')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d501174d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlation between features and target\n",
    "# First, let's create a subset without non-numeric columns and identifiers\n",
    "numeric_df = df_encoded.select_dtypes(include=['int64', 'float64'])\n",
    "numeric_df = numeric_df.drop(['loan_id', 'customer_id'], errors='ignore')  # Drop identifiers if present\n",
    "\n",
    "# Calculate correlation with target\n",
    "target_correlation = numeric_df.corr()['defaulted'].sort_values(ascending=False)\n",
    "\n",
    "# Plot top positive and negative correlations\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_n = 15\n",
    "\n",
    "# Top positive correlations\n",
    "plt.subplot(2, 1, 1)\n",
    "top_pos_corr = target_correlation.head(top_n+1)  # +1 to include defaulted itself\n",
    "top_pos_corr = top_pos_corr[1:]  # Remove defaulted\n",
    "sns.barplot(x=top_pos_corr.values, y=top_pos_corr.index)\n",
    "plt.title(f'Top {top_n} Positive Correlations with Default Status')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "\n",
    "# Top negative correlations\n",
    "plt.subplot(2, 1, 2)\n",
    "top_neg_corr = target_correlation.tail(top_n)\n",
    "sns.barplot(x=top_neg_corr.values, y=top_neg_corr.index)\n",
    "plt.title(f'Top {top_n} Negative Correlations with Default Status')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top correlated features\n",
    "print(\"Top positively correlated features:\")\n",
    "print(top_pos_corr)\n",
    "print(\"\\nTop negatively correlated features:\")\n",
    "print(top_neg_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f63500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicollinearity among features using Variance Inflation Factor (VIF)\n",
    "# We'll select a subset of important features to avoid computational issues\n",
    "\n",
    "# First, let's get the most correlated features (both positive and negative)\n",
    "important_features = list(pd.concat([top_pos_corr.head(10), top_neg_corr.head(10)]).index)\n",
    "\n",
    "# Add some of our engineered features\n",
    "important_features.extend([\n",
    "    'loan_to_income_ratio', 'debt_to_income_ratio', 'mobile_money_to_income_ratio',\n",
    "    'risk_factor', 'default_rate', 'socioeconomic_score'\n",
    "])\n",
    "\n",
    "# Remove duplicates and ensure 'defaulted' is not included\n",
    "important_features = list(set(important_features))\n",
    "if 'defaulted' in important_features:\n",
    "    important_features.remove('defaulted')\n",
    "\n",
    "# Limit to a reasonable number of features to avoid computational issues\n",
    "if len(important_features) > 20:\n",
    "    important_features = important_features[:20]\n",
    "\n",
    "# Create a DataFrame with selected features\n",
    "X_vif = numeric_df[important_features].copy()\n",
    "\n",
    "# Add a constant\n",
    "X_vif_const = sm.add_constant(X_vif)\n",
    "\n",
    "# Calculate VIF for each feature\n",
    "vif_data = pd.DataFrame()\n",
    "vif_data[\"Feature\"] = X_vif_const.columns\n",
    "vif_data[\"VIF\"] = [variance_inflation_factor(X_vif_const.values, i) for i in range(X_vif_const.shape[1])]\n",
    "\n",
    "# Sort by VIF value\n",
    "vif_data = vif_data.sort_values(\"VIF\", ascending=False)\n",
    "\n",
    "# Display VIF values\n",
    "print(\"Variance Inflation Factor (VIF) for selected features:\")\n",
    "print(vif_data)\n",
    "\n",
    "# Plot VIF values\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='VIF', y='Feature', data=vif_data)\n",
    "plt.title('Variance Inflation Factor (VIF) for Selected Features')\n",
    "plt.axvline(x=10, color='red', linestyle='--', label='VIF = 10 (Common threshold)')\n",
    "plt.axvline(x=5, color='orange', linestyle='--', label='VIF = 5 (Strict threshold)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f36ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Random Forest to identify important features\n",
    "\n",
    "# Prepare data for the model\n",
    "X = df_encoded.drop(['loan_id', 'customer_id', 'defaulted'], axis=1, errors='ignore')\n",
    "y = df_encoded['defaulted']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': rf.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot the top features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features = feature_importance.head(20)\n",
    "sns.barplot(x='Importance', y='Feature', data=top_features)\n",
    "plt.title('Top 20 Feature Importance from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display top important features\n",
    "print(\"Top 20 Most Important Features from Random Forest:\")\n",
    "print(feature_importance.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa329050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statistical tests to select features\n",
    "\n",
    "# ANOVA F-value for classification\n",
    "print(\"ANOVA F-value feature selection:\")\n",
    "selector_f = SelectKBest(f_classif, k=20)\n",
    "X_selected_f = selector_f.fit_transform(X, y)\n",
    "f_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'F-Score': selector_f.scores_,\n",
    "    'P-value': selector_f.pvalues_\n",
    "}).sort_values('F-Score', ascending=False)\n",
    "\n",
    "print(f_scores.head(20))\n",
    "\n",
    "# Mutual Information for feature selection\n",
    "print(\"\\nMutual Information feature selection:\")\n",
    "selector_mi = SelectKBest(mutual_info_classif, k=20)\n",
    "X_selected_mi = selector_mi.fit_transform(X, y)\n",
    "mi_scores = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'MI-Score': selector_mi.scores_,\n",
    "}).sort_values('MI-Score', ascending=False)\n",
    "\n",
    "print(mi_scores.head(20))\n",
    "\n",
    "# Visualize the selected features\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.barplot(x='F-Score', y='Feature', data=f_scores.head(20))\n",
    "plt.title('Top 20 Features by ANOVA F-Score')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "sns.barplot(x='MI-Score', y='Feature', data=mi_scores.head(20))\n",
    "plt.title('Top 20 Features by Mutual Information')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dc3735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical features\n",
    "\n",
    "# Define all numeric features\n",
    "numeric_features = df_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numeric_features = [feat for feat in numeric_features if feat not in ['loan_id', 'customer_id', 'defaulted']]\n",
    "\n",
    "# Create scaled versions of the data with different scalers\n",
    "df_scaled_standard = df_encoded.copy()\n",
    "df_scaled_minmax = df_encoded.copy()\n",
    "df_scaled_robust = df_encoded.copy()\n",
    "\n",
    "# Standard Scaling (Z-score normalization)\n",
    "scaler_standard = StandardScaler()\n",
    "df_scaled_standard[numeric_features] = scaler_standard.fit_transform(df_encoded[numeric_features])\n",
    "\n",
    "# Min-Max Scaling (0-1 normalization)\n",
    "scaler_minmax = MinMaxScaler()\n",
    "df_scaled_minmax[numeric_features] = scaler_minmax.fit_transform(df_encoded[numeric_features])\n",
    "\n",
    "# Robust Scaling (less sensitive to outliers)\n",
    "scaler_robust = RobustScaler()\n",
    "df_scaled_robust[numeric_features] = scaler_robust.fit_transform(df_encoded[numeric_features])\n",
    "\n",
    "# Compare distributions of original and scaled data for a few key features\n",
    "key_features_to_compare = [\n",
    "    'loan_to_income_ratio', 'monthly_income_kes', 'mobile_money_usage', \n",
    "    'risk_factor', 'socioeconomic_score'\n",
    "]\n",
    "\n",
    "# Create a long-format dataframe for easier plotting\n",
    "comparison_data = []\n",
    "\n",
    "for feature in key_features_to_compare:\n",
    "    # Original data\n",
    "    for value in df_encoded[feature]:\n",
    "        comparison_data.append({'Feature': feature, 'Scaling': 'Original', 'Value': value})\n",
    "    \n",
    "    # Standard scaled data\n",
    "    for value in df_scaled_standard[feature]:\n",
    "        comparison_data.append({'Feature': feature, 'Scaling': 'Standard', 'Value': value})\n",
    "    \n",
    "    # Min-Max scaled data\n",
    "    for value in df_scaled_minmax[feature]:\n",
    "        comparison_data.append({'Feature': feature, 'Scaling': 'Min-Max', 'Value': value})\n",
    "    \n",
    "    # Robust scaled data\n",
    "    for value in df_scaled_robust[feature]:\n",
    "        comparison_data.append({'Feature': feature, 'Scaling': 'Robust', 'Value': value})\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "# Visualize the distributions\n",
    "plt.figure(figsize=(15, 15))\n",
    "for i, feature in enumerate(key_features_to_compare):\n",
    "    plt.subplot(len(key_features_to_compare), 1, i+1)\n",
    "    feature_data = comparison_df[comparison_df['Feature'] == feature]\n",
    "    sns.boxplot(x='Scaling', y='Value', data=feature_data)\n",
    "    plt.title(f'Distribution of {feature} with Different Scaling Methods')\n",
    "    plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb62dea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify skewed features\n",
    "skewness = df_encoded[numeric_features].skew().sort_values(ascending=False)\n",
    "print(\"Skewness of numeric features:\")\n",
    "print(skewness)\n",
    "\n",
    "# Select highly skewed features for log transformation\n",
    "highly_skewed = skewness[abs(skewness) > 2].index.tolist()\n",
    "print(f\"\\nHighly skewed features (|skew| > 2):\")\n",
    "print(highly_skewed)\n",
    "\n",
    "# Apply log transformation to highly skewed features\n",
    "# Add a small constant to handle zeros\n",
    "df_log_transformed = df_encoded.copy()\n",
    "for feature in highly_skewed:\n",
    "    # Add 1 to handle zeros\n",
    "    df_log_transformed[f'{feature}_log'] = np.log1p(df_log_transformed[feature])\n",
    "\n",
    "# Compare original and log-transformed distributions\n",
    "plt.figure(figsize=(15, len(highly_skewed) * 5))\n",
    "for i, feature in enumerate(highly_skewed):\n",
    "    # Original distribution\n",
    "    plt.subplot(len(highly_skewed), 2, 2*i+1)\n",
    "    sns.histplot(df_encoded[feature], kde=True)\n",
    "    plt.title(f'Original {feature} (Skewness: {skewness[feature]:.2f})')\n",
    "    \n",
    "    # Log-transformed distribution\n",
    "    plt.subplot(len(highly_skewed), 2, 2*i+2)\n",
    "    sns.histplot(df_log_transformed[f'{feature}_log'], kde=True)\n",
    "    log_skewness = df_log_transformed[f'{feature}_log'].skew()\n",
    "    plt.title(f'Log-transformed {feature} (Skewness: {log_skewness:.2f})')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502450ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to reduce dimensionality\n",
    "\n",
    "# Select only numeric features for PCA\n",
    "X_pca = df_scaled_standard[numeric_features].copy()\n",
    "\n",
    "# Initialize and fit PCA\n",
    "pca = PCA(n_components=0.95)  # Capture 95% of variance\n",
    "X_pca_transformed = pca.fit_transform(X_pca)\n",
    "\n",
    "# Get explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "n_components = len(explained_variance)\n",
    "\n",
    "# Plot explained variance\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(1, n_components+1), explained_variance, alpha=0.7, label='Individual')\n",
    "plt.step(range(1, n_components+1), cumulative_variance, where='mid', label='Cumulative')\n",
    "plt.axhline(y=0.95, color='r', linestyle='-', alpha=0.5, label='95% Variance Threshold')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a dataframe with PCA components\n",
    "pca_df = pd.DataFrame(\n",
    "    data=X_pca_transformed,\n",
    "    columns=[f'PC{i+1}' for i in range(X_pca_transformed.shape[1])]\n",
    ")\n",
    "\n",
    "# Add the target variable back\n",
    "pca_df['defaulted'] = df_encoded['defaulted'].values\n",
    "\n",
    "# Plot the first two principal components\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='PC1', y='PC2', hue='defaulted', data=pca_df, palette=['#2ecc71', '#e74c3c'], alpha=0.7)\n",
    "plt.title('First Two Principal Components')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display component loadings\n",
    "if n_components <= 10:  # Only show if number of components is manageable\n",
    "    component_loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=numeric_features\n",
    "    )\n",
    "    \n",
    "    print(\"Top feature loadings for each principal component:\")\n",
    "    for i in range(min(5, n_components)):  # Show first 5 components or less\n",
    "        print(f\"\\nPrincipal Component {i+1} (Explains {explained_variance[i]*100:.2f}% of variance):\")\n",
    "        loadings = component_loadings[f'PC{i+1}'].abs().sort_values(ascending=False)\n",
    "        print(loadings.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c05048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine insights from different feature selection methods\n",
    "\n",
    "# Function to rank features based on multiple selection methods\n",
    "def get_feature_ranking(dataframes, rank_column_names, reverse_order=None):\n",
    "    \"\"\"\n",
    "    Combine rankings from multiple feature selection methods.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    dataframes : list of pandas DataFrames\n",
    "        Each DataFrame should contain feature selection results\n",
    "    rank_column_names : list of str\n",
    "        Name of the ranking column in each DataFrame\n",
    "    reverse_order : list of bool, optional\n",
    "        Whether to reverse the order for each method (True for methods where lower is better)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        Combined rankings with mean rank\n",
    "    \"\"\"\n",
    "    if reverse_order is None:\n",
    "        reverse_order = [False] * len(dataframes)\n",
    "    \n",
    "    # Initialize with all features\n",
    "    all_features = set()\n",
    "    for df in dataframes:\n",
    "        all_features.update(df['Feature'].values)\n",
    "    \n",
    "    # Create a dataframe to store ranks\n",
    "    ranks_df = pd.DataFrame({'Feature': list(all_features)})\n",
    "    \n",
    "    # Add ranks from each method\n",
    "    for i, (df, col_name, reverse) in enumerate(zip(dataframes, rank_column_names, reverse_order)):\n",
    "        # Create a ranking based on values\n",
    "        rank_col = f'Rank_{col_name}'\n",
    "        temp_df = df.copy()\n",
    "        if reverse:\n",
    "            temp_df[rank_col] = temp_df[col_name].rank(ascending=False)\n",
    "        else:\n",
    "            temp_df[rank_col] = temp_df[col_name].rank(ascending=True)\n",
    "        \n",
    "        # Merge with the ranks dataframe\n",
    "        ranks_df = ranks_df.merge(temp_df[['Feature', rank_col]], on='Feature', how='left')\n",
    "    \n",
    "    # Fill NaN with worst rank + 1\n",
    "    for col in ranks_df.columns:\n",
    "        if col.startswith('Rank_'):\n",
    "            worst_rank = ranks_df[col].max()\n",
    "            ranks_df[col] = ranks_df[col].fillna(worst_rank + 1)\n",
    "    \n",
    "    # Calculate mean rank\n",
    "    rank_columns = [col for col in ranks_df.columns if col.startswith('Rank_')]\n",
    "    ranks_df['Mean_Rank'] = ranks_df[rank_columns].mean(axis=1)\n",
    "    \n",
    "    # Sort by mean rank\n",
    "    ranks_df = ranks_df.sort_values('Mean_Rank')\n",
    "    \n",
    "    return ranks_df\n",
    "\n",
    "# Prepare DataFrames for ranking\n",
    "# 1. Correlation with target (absolute value)\n",
    "correlation_df = pd.DataFrame({\n",
    "    'Feature': numeric_df.columns,\n",
    "    'Correlation': numeric_df.corr()['defaulted'].abs()\n",
    "}).sort_values('Correlation', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c38bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create combined ranking\n",
    "feature_ranks = get_feature_ranking(\n",
    "    [correlation_df, feature_importance, f_scores, mi_scores],\n",
    "    ['Correlation', 'Importance', 'F-Score', 'MI-Score'],\n",
    "    [False, False, False, False]  # Higher is better for all methods\n",
    ")\n",
    "\n",
    "# Display top features\n",
    "print(\"Top 30 Features Based on Combined Ranking:\")\n",
    "print(feature_ranks.head(30))\n",
    "\n",
    "# Visualize top features\n",
    "plt.figure(figsize=(12, 10))\n",
    "top_features_to_plot = feature_ranks.head(20)\n",
    "sns.barplot(y='Feature', x='Mean_Rank', data=top_features_to_plot)\n",
    "plt.title('Top 20 Features by Combined Ranking')\n",
    "plt.xlabel('Mean Rank (Lower is Better)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b56374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 30 features from ranking\n",
    "top_ranked_features = feature_ranks['Feature'].head(30).tolist()\n",
    "\n",
    "# Add important domain-specific features that might not be in the top 30\n",
    "domain_specific_features = [\n",
    "    'loan_to_income_ratio', 'debt_to_income_ratio', \n",
    "    'mobile_money_to_income_ratio', 'default_rate',\n",
    "    'socioeconomic_score', 'risk_factor'\n",
    "]\n",
    "\n",
    "# Combine and remove duplicates\n",
    "final_features = list(set(top_ranked_features + domain_specific_features))\n",
    "\n",
    "# Ensure 'defaulted' is not in the feature list\n",
    "if 'defaulted' in final_features:\n",
    "    final_features.remove('defaulted')\n",
    "\n",
    "# Calculate number of features\n",
    "print(f\"Number of features in final set: {len(final_features)}\")\n",
    "print(\"\\nFinal Feature Set:\")\n",
    "print(final_features)\n",
    "\n",
    "# Create the final dataset for modeling\n",
    "final_df = df_encoded[final_features + ['defaulted']].copy()\n",
    "\n",
    "# Display sample of the final dataset\n",
    "print(\"\\nSample of Final Dataset:\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833a93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Full dataset with all engineered features\n",
    "processed_path_full = \"../data/processed/kenyan_loan_default_engineered_full.csv\"\n",
    "df_encoded.to_csv(processed_path_full, index=False)\n",
    "print(f\"Full dataset with all engineered features saved to: {processed_path_full}\")\n",
    "\n",
    "# 2. Selected feature dataset\n",
    "processed_path_selected = \"../data/processed/kenyan_loan_default_engineered_selected.csv\"\n",
    "final_df.to_csv(processed_path_selected, index=False)\n",
    "print(f\"Selected feature dataset saved to: {processed_path_selected}\")\n",
    "\n",
    "# 3. Scaled dataset (Standard scaling)\n",
    "processed_path_scaled = \"../data/processed/kenyan_loan_default_engineered_scaled.csv\"\n",
    "df_scaled_standard[final_features + ['defaulted']].to_csv(processed_path_scaled, index=False)\n",
    "print(f\"Scaled dataset saved to: {processed_path_scaled}\")\n",
    "\n",
    "# 4. PCA transformed dataset\n",
    "processed_path_pca = \"../data/processed/kenyan_loan_default_pca.csv\"\n",
    "pca_df.to_csv(processed_path_pca, index=False)\n",
    "print(f\"PCA transformed dataset saved to: {processed_path_pca}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
